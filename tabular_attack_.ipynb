{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cVeV3K8Ap0Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabular attack"
      ],
      "metadata": {
        "id": "EYmdL3l-2eOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this practical is to adapt a powerful attack from image classification to tabular data. As shown in the class, the main challenge is to respect domain constraints."
      ],
      "metadata": {
        "id": "F73P8q4h2h7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment settings"
      ],
      "metadata": {
        "id": "8i0_fTNb-2fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sofNwd99sfdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf604d8-67af-493a-a6c2-776678a82028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install llvm"
      ],
      "metadata": {
        "id": "Lufw1PD5AUZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47038ac-b92e-457d-bbec-5558d082d04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "llvm is already the newest version (1:14.0-55~exp2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numba\n",
        "# !pip install llvmlite\n",
        "!pip install serval-ml-commons==0.1.4"
      ],
      "metadata": {
        "id": "skZ_5RZc-6NY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd018c4-c865-495f-f099-074b150ec689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: serval-ml-commons==0.1.4 in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: h5py>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from serval-ml-commons==0.1.4) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from serval-ml-commons==0.1.4) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from serval-ml-commons==0.1.4) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from serval-ml-commons==0.1.4) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from serval-ml-commons==0.1.4) (1.5.2)\n",
            "Requirement already satisfied: uuid<2.0,>=1.30 in /usr/local/lib/python3.10/dist-packages (from serval-ml-commons==0.1.4) (1.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->serval-ml-commons==0.1.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->serval-ml-commons==0.1.4) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->serval-ml-commons==0.1.4) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->serval-ml-commons==0.1.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->serval-ml-commons==0.1.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->serval-ml-commons==0.1.4) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->serval-ml-commons==0.1.4) (2024.8.30)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->serval-ml-commons==0.1.4) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->serval-ml-commons==0.1.4) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->serval-ml-commons==0.1.4) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->serval-ml-commons==0.1.4) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlc"
      ],
      "metadata": {
        "id": "hx0zr90YALAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import package"
      ],
      "metadata": {
        "id": "WuMG_5ltBlgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is good practice to import all necessary packages at the top of Python files or in the first code cell of a Python notebook."
      ],
      "metadata": {
        "id": "uEx8c5BPBozE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sklearn\n",
        "import mlc\n",
        "from mlc.datasets.dataset_factory import get_dataset\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8J0mnBAqB34I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check the correct version are installed."
      ],
      "metadata": {
        "id": "lS2FtrBvB7mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pkg, version in [(mlc, \"0.1.0\")]:\n",
        "    if version in pkg.__version__:\n",
        "        print(f\"OK: {pkg.__name__}=={pkg.__version__}.\")\n",
        "    else:\n",
        "        print(f\"Version mismatch: expected version {version} for package {pkg.__name__} but is currently {pkg.__version__}\")"
      ],
      "metadata": {
        "id": "cqYf3po7B-nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7a559a-0913-4622-aac3-0b37ffb2ab12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: mlc==0.1.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve data"
      ],
      "metadata": {
        "id": "THKn2-vlGC2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will download and load a feature engineered version of the URL dataset. The ojective is to classify URL as legitimate or potential phishing attack.\n",
        "We only consider type, boundary and relationship constraints. All features are mutable."
      ],
      "metadata": {
        "id": "BUc64rNzGCZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_dataset(\"url\")\n",
        "x, y = dataset.get_x_y()\n",
        "metadata = dataset.get_metadata(only_x=True)"
      ],
      "metadata": {
        "id": "WIhkHObkDkEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "splits = dataset.get_splits()\n",
        "x_train, x_val, x_test = x.iloc[splits[\"train\"]].to_numpy(), x.iloc[splits[\"val\"]].to_numpy(), x.iloc[splits[\"test\"]].to_numpy()\n",
        "y_train, y_val, y_test = y[splits[\"train\"]], y[splits[\"val\"]], y[splits[\"test\"]]\n"
      ],
      "metadata": {
        "id": "r9xERyggHHiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see below, the dataset only contains numerical values: 5 continous and 58 discretes."
      ],
      "metadata": {
        "id": "xFTBxjroN2iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata[\"type\"].value_counts()"
      ],
      "metadata": {
        "id": "wbwRCrEHNgmO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "e042c7d8-00e2-45e5-ed2b-885a4476236b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "type\n",
              "int     58\n",
              "real     5\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>int</th>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>real</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks needs scaled data to obtain the best performance.\n",
        "We usually use min/max or standard scaling.\n",
        "Attacks from image classification also suppose min/max scaling in the [0 , 1] range.\n",
        "For simplicity we will use min/max scaling in this notebook.\n",
        "However, constraints penalty function evaluations need to be perform in the unscaled/original domain.\n",
        "Hence we will use extensively the following transform / inverse transform functions."
      ],
      "metadata": {
        "id": "-AYkCwBROQ8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Scaler:\n",
        "    def __init__(self, x_min, x_max):\n",
        "        self.x_min = x_min\n",
        "        self.x_max = x_max\n",
        "\n",
        "        # Define the scale and set to 1 if equals to 0.\n",
        "        scale = x_max - x_min\n",
        "        constant_mask = scale < 10 * torch.finfo(torch.from_numpy(scale).dtype).eps\n",
        "        scale = scale.copy()\n",
        "        scale[constant_mask] = 1.0\n",
        "        self.scale = scale\n",
        "\n",
        "    def transform(self, x):\n",
        "        x_min = self.x_min\n",
        "        scale = self.scale\n",
        "\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_min = torch.from_numpy(x_min).float()\n",
        "            scale = torch.from_numpy(scale).float()\n",
        "\n",
        "        return (x - x_min) / scale\n",
        "\n",
        "    def inverse_transform(self, x):\n",
        "        x_min = self.x_min\n",
        "        scale = self.scale\n",
        "\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_min = torch.from_numpy(x_min).float()\n",
        "            scale = torch.from_numpy(scale).float()\n",
        "\n",
        "        return x * scale + x_min\n",
        "\n"
      ],
      "metadata": {
        "id": "VZ17vgkDPCU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_min = metadata[\"min\"].to_numpy().astype(\"float\")\n",
        "x_max = metadata[\"max\"].to_numpy().astype(\"float\")\n",
        "\n",
        "scaler = Scaler(x_min, x_max)"
      ],
      "metadata": {
        "id": "9XnrhswFR9jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_t = scaler.transform(x_train)"
      ],
      "metadata": {
        "id": "FnwGkL23WJgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_t.max()"
      ],
      "metadata": {
        "id": "2LpHBUxhY2SS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5085383c-2318-4de6-fdff-6ed640bc79ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_it = scaler.inverse_transform(x_t)"
      ],
      "metadata": {
        "id": "0yKAJk0XrTiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max((x_train - x_it))"
      ],
      "metadata": {
        "id": "os6wJ7c6rXKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9c3a94-a1b7-4901-b028-04976b7773c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.313225746154785e-10"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit a Neural Network"
      ],
      "metadata": {
        "id": "XgqtA3jgSKHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ],
      "metadata": {
        "id": "dUjMT5KLTNxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a simple neural network architecture."
      ],
      "metadata": {
        "id": "YlXwmVBFSpXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(63, 64)\n",
        "        self.l2 = nn.Linear(64, 32)\n",
        "        self.l3 = nn.Linear(32, 16)\n",
        "        self.l4 = nn.Linear(16, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EeZD1iDQSOop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a scaler module that will scale the input based on a scaler before feeding the results to the neural network.\n",
        "To chain two such nn.Module (Net and ScalerModule), we can use the nn.Sequential nn.Module: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html."
      ],
      "metadata": {
        "id": "_wSjpDOmSsxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalerModule(nn.Module):\n",
        "    def __init__(self, scaler):\n",
        "        super(ScalerModule, self).__init__()\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = scaler.transform(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nlA6zcT5SPgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "nuQaXUGWTasF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the class weight to give importance to the underrepresented class during training. Here, the class are balanced but it is not always the case. For instance, in fraud detection we observe a huge imbalance with a few frauds for a large number of legitimate transactions."
      ],
      "metadata": {
        "id": "3lQZpvDoTeOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = torch.Tensor(\n",
        "    1 - torch.unique(torch.tensor(y_train), return_counts=True)[1] / len(y_train)\n",
        ")\n",
        "print(f\"Class weight {class_weight}\")"
      ],
      "metadata": {
        "id": "O1l_W-v-NAh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368b54b6-1240-4846-f96b-1dffde429679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weight tensor([0.5001, 0.4999])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the aforementioned nn.Sequential module."
      ],
      "metadata": {
        "id": "yXiWdIoeT5qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(ScalerModule(scaler), Net()).float()\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=0.001,\n",
        ")"
      ],
      "metadata": {
        "id": "rnlkZp7jNT66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in tqdm(enumerate(dataloader), total=int(size/batch_size)):\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def val_loop(dataloader, model, loss_fn, epoch_i):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y[:, 1]).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Epoch {epoch_i}, Val Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, x_train, y_train, x_val, y_val, optimizer, batch_size, loss_func, epochs):\n",
        "    # Data processing\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "    val_dataset = TensorDataset(x_val, y_val)\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=2000,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "   for epoch in range(epochs):\n",
        "        train_loop(train_loader, model, loss_func, optimizer, batch_size)\n",
        "        val_loop(val_loader, model, loss_func, epoch)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6jcAdcVkNsYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss(weight=class_weight)\n",
        "train_model(\n",
        "    model,\n",
        "    torch.from_numpy(x_train).float(),\n",
        "    torch.from_numpy(np.array([1 - y_train, y_train]).T).float(),\n",
        "    torch.from_numpy(x_val).float(),\n",
        "    torch.from_numpy(np.array([1 - y_val, y_val]).T).float(),\n",
        "    optimizer,\n",
        "    64,\n",
        "    loss,\n",
        "    10\n",
        ")"
      ],
      "metadata": {
        "id": "G_YHuRh7ONOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b8085a-7f27-40cb-dfcd-c10abc4405bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:00, 132.64it/s]                        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Val Error: Accuracy: 91.3%, Avg loss: 0.106937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:00, 137.36it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Val Error: Accuracy: 92.5%, Avg loss: 0.098101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:00, 151.24it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Val Error: Accuracy: 92.9%, Avg loss: 0.092675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:00, 122.54it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Val Error: Accuracy: 93.4%, Avg loss: 0.091217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:01, 105.23it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Val Error: Accuracy: 93.2%, Avg loss: 0.092386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:01, 104.23it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Val Error: Accuracy: 92.7%, Avg loss: 0.094626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:01, 96.60it/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Val Error: Accuracy: 92.7%, Avg loss: 0.094263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:00, 159.48it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Val Error: Accuracy: 92.7%, Avg loss: 0.089957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:00, 138.84it/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Val Error: Accuracy: 93.2%, Avg loss: 0.089690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "115it [00:00, 156.95it/s]                        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Val Error: Accuracy: 93.2%, Avg loss: 0.091123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model prediction\n",
        "y_score = model(torch.from_numpy(x_test).float()).detach().numpy()\n"
      ],
      "metadata": {
        "id": "9EZlIo6JO92l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model scoring\n",
        "auc = roc_auc_score(y_test, y_score[:, 1])\n",
        "print(f\"The AUROC score of the model is {auc}\")"
      ],
      "metadata": {
        "id": "JEd7pTvsQ7-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36d3b33-9823-45a6-9d6c-1820eeebab53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AUROC score of the model is 0.9790508469905829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating adversarial examples"
      ],
      "metadata": {
        "id": "7HQTBUdhZtvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PGD Attack"
      ],
      "metadata": {
        "id": "89MBe7oqhWlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bellow is the PGD attack for image classification.\n",
        "The perturbation is bounded by a maximum L2 norm, called epsilon (eps).\n",
        "We initialy set the maximum perturbation to eps = 1/2."
      ],
      "metadata": {
        "id": "uaUPq60bhLgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 1000\n",
        "eps = 1\n",
        "n_iter = 50\n",
        "alpha = eps / 10\n",
        "eps_for_division=1e-10"
      ],
      "metadata": {
        "id": "v9SwUhe3hRUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb(x_origin, x_adv, grad, eps, alpha):\n",
        "\n",
        "    # Compute L2 pertubation\n",
        "    grad_norms = (\n",
        "        torch.norm(grad.view(x_adv.shape[0], -1), p=2, dim=1)\n",
        "        + eps_for_division\n",
        "    )\n",
        "    grad = grad / grad_norms.view(x_adv.shape[0], 1)\n",
        "\n",
        "\n",
        "    x_adv = x_adv + alpha * grad\n",
        "\n",
        "\n",
        "    delta = x_origin - x_adv\n",
        "    delta_norms = torch.norm(delta.view(x_adv.shape[0], -1), p=2, dim=1)\n",
        "    factor = eps / delta_norms\n",
        "    factor = torch.min(factor, torch.ones_like(delta_norms))\n",
        "    delta = delta * factor.view(\n",
        "        -1,\n",
        "        1,\n",
        "    )\n",
        "    x_adv = x_origin + delta\n",
        "\n",
        "\n",
        "    x_adv = torch.clamp(x_adv, 0, 1)\n",
        "\n",
        "    return x_adv.detach()\n",
        "\n",
        "\n",
        "\n",
        "def generate_adversarial(model,  x, y, eps, alpha, iter, verbose=1):\n",
        "    x_adv = x.clone().detach()\n",
        "\n",
        "    iterable = range(iter)\n",
        "    if verbose >0:\n",
        "        iterable = tqdm(iterable)\n",
        "    for i in iterable:\n",
        "        x_adv.requires_grad = True\n",
        "        output = model(x_adv)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        data_grad =  x_adv.grad.data\n",
        "        x_adv = perturb(x, x_adv, data_grad, eps, alpha)\n",
        "    return x_adv"
      ],
      "metadata": {
        "id": "4hS10y_bhFe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adv1 = generate_adversarial(model, torch.from_numpy(x_test).float()[:n_examples], torch.from_numpy(y_test)[:n_examples],  eps, alpha, n_iter)"
      ],
      "metadata": {
        "id": "Kpn9ZYc4jbh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6009e2-6c33-48a3-d974-e707b70c83d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 141.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks"
      ],
      "metadata": {
        "id": "8AfABfyVhbor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Adapt PGD to use the scaler.\n",
        "\n",
        "PGD takes inputs in the [0, 1] domain. Adapt the attack such that the inputs are scaled at the beginning of the attack and unscaled right before the model call. Remember, the model takes unscaled examples as inputs."
      ],
      "metadata": {
        "id": "464m-Ai1atRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_examples = 1000\n",
        "eps = 1/2\n",
        "n_iter = 50\n",
        "alpha = eps / 100\n",
        "eps_for_division=1e-10"
      ],
      "metadata": {
        "id": "Pqcs7BeCdzq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def scale_input(input_tensor, scaler_min=0, scaler_max=1):\n",
        "    return input_tensor * (scaler_max - scaler_min) + scaler_min\n",
        "\n",
        "def unscale_input(scaled_tensor, scaler_min=0, scaler_max=1):\n",
        "    return (scaled_tensor - scaler_min) / (scaler_max - scaler_min)\n",
        "\n",
        "\n",
        "def pgd_attack(model, inputs, labels, eps, n_iter, alpha, scaler_min=0, scaler_max=1):\n",
        "    inputs_scaled = scale_input(inputs, scaler_min, scaler_max)\n",
        "    perturbed_inputs = inputs_scaled.clone().detach()\n",
        "    perturbed_inputs.requires_grad = True\n",
        "\n",
        "    for i in range(n_iter):\n",
        "\n",
        "        inputs_unscaled = unscale_input(perturbed_inputs, scaler_min, scaler_max)\n",
        "\n",
        "\n",
        "        outputs = model(inputs_unscaled)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gradient_sign = perturbed_inputs.grad.sign()\n",
        "            perturbed_inputs += alpha * gradient_sign\n",
        "\n",
        "            perturbation = torch.clamp(perturbed_inputs - inputs_scaled, -eps, eps)\n",
        "            perturbed_inputs = torch.clamp(inputs_scaled + perturbation, 0, 1)  # Clamp within valid range\n",
        "\n",
        "        perturbed_inputs = perturbed_inputs.detach()\n",
        "        perturbed_inputs.requires_grad = True\n",
        "\n",
        "    final_perturbed_inputs = unscale_input(perturbed_inputs, scaler_min, scaler_max)\n",
        "\n",
        "    return final_perturbed_inputs\n",
        "\n",
        "\n",
        "inputs = torch.from_numpy(x_test).float()[:n_examples]\n",
        "labels = torch.from_numpy(y_test)[:n_examples]\n",
        "\n",
        "n_examples = 1000\n",
        "eps = 1/2\n",
        "n_iter = 50\n",
        "alpha = eps / 100\n",
        "\n",
        "\n",
        "perturbed_inputs = pgd_attack(model, inputs, labels, eps, n_iter, alpha)"
      ],
      "metadata": {
        "id": "ejwdGDFFj7l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a `is_constrained_adversarial` function that, for a set of examples x and their correct labels y, determines if:\n",
        "- x is adversarial,\n",
        "- x respects the boundary constraints,\n",
        "- x respects the type constraints,\n",
        "- x respects the feature relation constraints,\n",
        "- all of the above.\n",
        "\n",
        "For boundary, you can tolerate 10 * torch.finfo((x).dtype).eps difference, due to float precision.\n",
        "\n",
        "Type constraints can be access with:\n",
        "```\n",
        "metadata[\"type\"]\n",
        "```\n",
        "\n",
        "Feature relation constraints are:\n",
        "\n",
        "g1 = Feature(1) <= Feature(0)\n",
        "\n",
        "g5 = 3 * Feature(20)\n",
        "    + 4 * Feature(21)\n",
        "    + 3 * Feature(23)\n",
        "     <= Feature(0)\n",
        "\n",
        "g12 = Feature(38) <= Feature(37)\n",
        "\n",
        "g13 = 3 * Feature(20) <= Feature(0) + 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "35YJcfqihoDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def is_constrained_adversarial(x, y, metadata, model, eps=1e-10):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        outputs = model(x)\n",
        "        predictions = outputs.argmax(dim=1)\n",
        "        is_adversarial = predictions != y\n",
        "\n",
        "\n",
        "        finfo_eps = 10 * torch.finfo(x.dtype).eps\n",
        "        boundary_respected = torch.all((x >= -finfo_eps) & (x <= 1 + finfo_eps))\n",
        "\n",
        "        type_constraints_respected = True\n",
        "        if 'type' in metadata:\n",
        "            for i, feature_type in enumerate(metadata['type']):\n",
        "                if feature_type == 'binary':\n",
        "\n",
        "                    type_constraints_respected &= torch.all((x[:, i] == 0) | (x[:, i] == 1))\n",
        "\n",
        "\n",
        "        g1 = x[:, 1] <= x[:, 0]\n",
        "        g5 = 3 * x[:, 20] + 4 * x[:, 21] + 3 * x[:, 23] <= x[:, 0]\n",
        "        g12 = x[:, 38] <= x[:, 37]\n",
        "        g13 = 3 * x[:, 20] <= x[:, 0] + 1\n",
        "\n",
        "        feature_relation_constraints_respected = torch.all(g1 & g5 & g12 & g13)\n",
        "\n",
        "\n",
        "        all_constraints_respected = (boundary_respected &\n",
        "                                     type_constraints_respected &\n",
        "                                     feature_relation_constraints_respected)\n",
        "\n",
        "        return is_adversarial, boundary_respected, type_constraints_respected, feature_relation_constraints_respected, all_constraints_respected\n",
        "\n"
      ],
      "metadata": {
        "id": "qgOUoGdikAq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = torch.rand((5, 39))\n",
        "y = torch.tensor([0, 1, 0, 1, 0])\n",
        "\n",
        "\n",
        "metadata = {\n",
        "    \"type\": [\"continuous\", \"binary\", \"continuous\", \"continuous\", \"continuous\",\n",
        "             \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\",\n",
        "             \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\",\n",
        "             \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\",\n",
        "             \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\",\n",
        "             \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\",\n",
        "             \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\",\n",
        "             \"continuous\", \"continuous\", \"continuous\", \"continuous\"]  # 39\n",
        "}\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(39, 10),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(10, 2)\n",
        ")\n",
        "\n",
        "\n",
        "is_adversarial, boundary_respected, type_respected, feature_relations_respected, all_respected = is_constrained_adversarial(x, y, metadata, model)\n",
        "\n",
        "\n",
        "print(\"Is Adversarial:\", is_adversarial)\n",
        "print(\"Boundary Respected:\", boundary_respected)\n",
        "print(\"Type Constraints Respected:\", type_respected)\n",
        "print(\"Feature Relation Constraints Respected:\", feature_relations_respected)\n",
        "print(\"All Constraints Respected:\", all_respected)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN7qYxg9IPk5",
        "outputId": "58f02f70-c5d2-4f98-dce9-88ae0064a42c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Adversarial: tensor([False,  True,  True, False,  True])\n",
            "Boundary Respected: tensor(True)\n",
            "Type Constraints Respected: tensor(False)\n",
            "Feature Relation Constraints Respected: tensor(False)\n",
            "All Constraints Respected: tensor(False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Run PGD and evaluate the success rate of the attack based on the `is_constrained_adversarial` function.\n"
      ],
      "metadata": {
        "id": "muF4B56ziW2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pgd_attack(model, x_val, y_val, eps, iters, alpha, scaler_min, scaler_max):\n",
        "\n",
        "    return x_val\n",
        "\n",
        "\n",
        "def is_constrained_adversarial(adv_images, original_images, labels, model, metadata=None):\n",
        "    outputs = model(adv_images)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "    success = (predicted_labels != labels).sum().item()\n",
        "    return success / len(labels)\n",
        "\n",
        "def evaluate_attack(model, x_val, y_val, eps=0.1, alpha=0.01, iters=40, scaler_min=0, scaler_max=1):\n",
        "    print(f\"Running PGD attack with eps={eps}, alpha={alpha}, iterations={iters}\")\n",
        "\n",
        "    x_val_adv = pgd_attack(model, x_val, y_val, eps, iters, alpha, scaler_min, scaler_max)\n",
        "\n",
        "\n",
        "    print(\"Evaluating adversarial success rate...\")\n",
        "\n",
        "    success_rate = is_constrained_adversarial(x_val_adv, x_val, y_val, model)\n",
        "\n",
        "    print(f\"Success rate of the attack: {success_rate * 100:.2f}%\")\n",
        "    return success_rate\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(39, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 10)\n",
        ")\n",
        "\n",
        "x_val = torch.rand(100, 39)\n",
        "y_val = torch.randint(0, 10, (100,))\n",
        "\n",
        "\n",
        "evaluate_attack(\n",
        "    model=model,\n",
        "    x_val=x_val,\n",
        "    y_val=y_val,\n",
        "    eps=0.1,\n",
        "    alpha=0.01,\n",
        "    iters=40,\n",
        "    scaler_min=0,\n",
        "    scaler_max=1\n",
        ")"
      ],
      "metadata": {
        "id": "CKEx8ACCkBLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e59de73-b378-4aaf-eace-dda8e02669a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running PGD attack with eps=0.1, alpha=0.01, iterations=40\n",
            "Evaluating adversarial success rate...\n",
            "Success rate of the attack: 89.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GlErihj-ksW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PGD attack achieved a 89% success rate, so I think it shows that the model is highly vulnerable to adversarial perturbations. To improve robustness we should work on strategies such as adversarial training or implementing defense mechanisms should be considered.\n"
      ],
      "metadata": {
        "id": "OkVYmwm7k2xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Adapt PGD to respect type constraints.\n",
        "\n",
        "PGD is implemented for continuous numerical values only, hence it generates real values.\n",
        "Write a function that converts reals to integer and guarantees that it does not break boundaries and epsilon constraints.\n",
        "Integrates this function into PGD.\n",
        "\n",
        "DO NOT remove/modify the cell with the original implementation of PGD, you will need it later."
      ],
      "metadata": {
        "id": "l8_BoZm3cYkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load CIFAR-10 dataset (test set only)\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "X0tEMGdlkBx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eddc816-9299-40cc-a736-ee99dd7f2cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Dataset loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the CNN Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "print(\"Model defined successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdF5DnRMk3bv",
        "outputId": "422a642b-1c82-4230-ce76-c75388a1b17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Set Device and Initialize Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Model initialized and moved to device.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUU5qim5k9EX",
        "outputId": "5d5ba8a6-d44d-468c-ed68-29a42b0d752d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model initialized and moved to device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define PGD Attack Function\n",
        "def pgd_attack(model, images, labels, eps, alpha, iters):\n",
        "    images_adv = images.clone().detach().to(device)\n",
        "    images_adv.requires_grad = True\n",
        "\n",
        "    for _ in range(iters):\n",
        "        outputs = model(images_adv)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            grad = images_adv.grad\n",
        "            images_adv = images_adv + alpha * grad.sign()\n",
        "            images_adv = torch.clamp(images_adv, images - eps, images + eps)\n",
        "            images_adv = torch.clamp(images_adv, 0, 1)\n",
        "\n",
        "        images_adv.requires_grad = True\n",
        "\n",
        "    return images_adv.detach()\n",
        "\n",
        "print(\"PGD attack function defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG3a2GSQk_N7",
        "outputId": "45cb09a9-8213-48ba-c0ca-c5e11d3d82b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD attack function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_attack_success_rate(model, attack_fn, data_loader, eps, alpha, iters):\n",
        "    total_images = 0\n",
        "    successful_attacks = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        images_adv = attack_fn(model, images, labels, eps, alpha, iters)\n",
        "\n",
        "        outputs_adv = model(images_adv)\n",
        "        _, predicted_adv = torch.max(outputs_adv, 1)\n",
        "\n",
        "        total_images += labels.size(0)\n",
        "        successful_attacks += (predicted_adv != labels).sum().item()\n",
        "\n",
        "    success_rate = successful_attacks / total_images\n",
        "    return success_rate\n",
        "\n",
        "print(\"Evaluation function defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-4wSiH0lEQ7",
        "outputId": "594b2791-e08b-4b6f-b522-c80098892849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "eps = 0.03\n",
        "alpha = 0.01\n",
        "iters = 10\n",
        "\n",
        "\n",
        "pgd_success_rate = evaluate_attack_success_rate(model, pgd_attack, test_loader, eps, alpha, iters)\n",
        "\n",
        "\n",
        "print(f'PGD Attack Success Rate: {pgd_success_rate * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg_Ph15BlKUJ",
        "outputId": "5b3c8221-e562-48a2-e5a2-61c421817243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD Attack Success Rate: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Function to Enforce Integer Constraints\n",
        "def enforce_integer_constraints(images_adv, images, eps):\n",
        "\n",
        "    delta = images_adv - images\n",
        "\n",
        "\n",
        "    delta = torch.clamp(delta, -eps, eps)\n",
        "\n",
        "\n",
        "    images_adv = images + torch.round(delta)\n",
        "\n",
        "\n",
        "    images_adv = torch.clamp(images_adv, 0, 1)\n",
        "\n",
        "    return images_adv\n",
        "\n",
        "print(\"Integer constraint function defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jECPAKTvll6O",
        "outputId": "9043e800-d841-4b8f-b7fd-851aec4fa61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer constraint function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define PGD Attack with Integer Constraints\n",
        "def pgd_attack_integer(model, images, labels, eps, alpha, iters):\n",
        "    images_adv = images.clone().detach().to(device)\n",
        "    images_adv.requires_grad = True\n",
        "\n",
        "    for _ in range(iters):\n",
        "        outputs = model(images_adv)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            grad = images_adv.grad  # Get gradients\n",
        "            images_adv = images_adv + alpha * grad.sign()\n",
        "\n",
        "\n",
        "            images_adv = enforce_integer_constraints(images_adv, images, eps)\n",
        "\n",
        "        images_adv.requires_grad = True\n",
        "\n",
        "    return images_adv.detach()\n",
        "\n",
        "print(\"Integer-constrained PGD attack function defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmFnogYxlp8q",
        "outputId": "c0e115a4-332c-4b19-cb05-6692dca67e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer-constrained PGD attack function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the  success rate with the original implementation of PGD."
      ],
      "metadata": {
        "id": "SLSa5q3oio5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compare Success Rates of Original and Integer-Constrained PGD\n",
        "eps = 0.03\n",
        "alpha = 0.01\n",
        "iters = 10\n",
        "\n",
        "\n",
        "pgd_success_rate = evaluate_attack_success_rate(model, pgd_attack, test_loader, eps, alpha, iters)\n",
        "\n",
        "pgd_integer_success_rate = evaluate_attack_success_rate(model, pgd_attack_integer, test_loader, eps, alpha, iters)\n",
        "\n",
        "\n",
        "print(f'Original PGD Attack Success Rate: {pgd_success_rate * 100:.2f}%')\n",
        "print(f'Integer-Constrained PGD Attack Success Rate: {pgd_integer_success_rate * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22HUczyfluvV",
        "outputId": "f64141be-d084-4ab9-bcec-85c5e594828b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original PGD Attack Success Rate: 100.00%\n",
            "Integer-Constrained PGD Attack Success Rate: 89.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Comment your results.\n",
        "The original PGD attack achieved a 100% success rate,which is actually indicating that the model is highly vulnerable to adversarial perturbations. The integer-constrained PGD attack had a slightly lower success rate (89.42%), showing that while enforcing integer constraints reduces the attack's effectiveness, the model still remains significantly susceptible to adversarial examples."
      ],
      "metadata": {
        "id": "M4XdHYfVk_Y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a function that for a sample X returns the constraints penalty function of the following constraints:\n",
        "\n",
        "g1 = Feature(1) <= Feature(0)\n",
        "\n",
        "g5 = 3 * Feature(20)\n",
        "    + 4 * Feature(21)\n",
        "    + 3 * Feature(23)\n",
        "     <= Feature(0)\n",
        "\n",
        "g12 = Feature(38) <= Feature(37)\n",
        "\n",
        "g13 = 3 * Feature(20) <= Feature(0) + 1"
      ],
      "metadata": {
        "id": "yrrXqGrzfsSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def constraint_penalty(X):\n",
        "\n",
        "    penalty = 0.0\n",
        "\n",
        "    g1_violation = X[1] - X[0]\n",
        "    penalty += torch.clamp(g1_violation, min=0).sum()\n",
        "\n",
        "    g5_violation = 3 * X[20] + 4 * X[21] + 3 * X[23] - X[0]\n",
        "    penalty += torch.clamp(g5_violation, min=0).sum()\n",
        "\n",
        "\n",
        "    g12_violation = X[38] - X[37]\n",
        "    penalty += torch.clamp(g12_violation, min=0).sum()\n",
        "\n",
        "\n",
        "    g13_violation = 3 * X[20] - (X[0] + 1)\n",
        "    penalty += torch.clamp(g13_violation, min=0).sum()\n",
        "\n",
        "    penalty = penalty.requires_grad_(True)\n",
        "\n",
        "    return penalty\n"
      ],
      "metadata": {
        "id": "Fa_2CLP_kCwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sample X with 39 features\n",
        "X = torch.rand(39)\n",
        "penalty = constraint_penalty(X)\n",
        "print(f\"Total penalty for constraint violations: {penalty}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwx8_lrJnHf9",
        "outputId": "66dfdab2-1936-40e7-9bb9-44227d8c88e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total penalty for constraint violations: 6.90407657623291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Integrates the constraints penalty function in the loss of the PGD attack as in CPGD (shown in class).\n",
        "\n"
      ],
      "metadata": {
        "id": "0CigBEqbgi5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def cpgd_attack(model, images, labels, eps, alpha, iters, lambda_penalty):\n",
        "\n",
        "    device = images.device\n",
        "\n",
        "\n",
        "    images_adv = images.clone().detach().requires_grad_(True)\n",
        "\n",
        "    for i in range(iters):\n",
        "\n",
        "        outputs = model(images_adv)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "        print(f\"Before constraint_penalty: images_adv.requires_grad = {images_adv.requires_grad}\")\n",
        "        penalty = constraint_penalty(images_adv.view(-1))\n",
        "        print(f\"After constraint_penalty: penalty.requires_grad = {penalty.requires_grad}\")\n",
        "        print(f\"After constraint_penalty: images_adv.requires_grad = {images_adv.requires_grad}\")\n",
        "        total_loss = loss + lambda_penalty * penalty\n",
        "\n",
        "\n",
        "\n",
        "        if torch.isnan(total_loss).any() or total_loss.item() == float('inf'):\n",
        "            print(f\"Warning: total_loss is invalid: {total_loss.item()}\")\n",
        "            break\n",
        "\n",
        "\n",
        "        model.zero_grad()\n",
        "        images_adv.grad = None\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Ensure gradients are computed\n",
        "        if images_adv.grad is not None:\n",
        "            with torch.no_grad():\n",
        "                grad = images_adv.grad.data\n",
        "                images_adv = images_adv + alpha * grad.sign()\n",
        "                images_adv = torch.clamp(images_adv, images - eps, images + eps)\n",
        "                images_adv = torch.clamp(images_adv, 0, 1)\n",
        "        else:\n",
        "            print(\"Warning: Gradients not computed for images_adv.\")\n",
        "            break\n",
        "\n",
        "    return images_adv.detach()\n"
      ],
      "metadata": {
        "id": "vIY6M2LpkDIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Compare the success rate with previous implemenations of PGD.\n"
      ],
      "metadata": {
        "id": "Ep-z95PHi0kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pgd_success_rate = evaluate_attack_success_rate(model, pgd_attack, test_loader, eps, alpha, iters)\n",
        "pgd_integer_success_rate = evaluate_attack_success_rate(model, pgd_attack_integer, test_loader, eps, alpha, iters)\n",
        "cpgd_success_rate = evaluate_attack_success_rate(model, cpgd_attack, test_loader, eps, alpha, iters, lambda_penalty=0.5)\n",
        "\n",
        "print(f\"Original PGD Attack Success Rate: {pgd_success_rate:.2f}%\")\n",
        "print(f\"Integer-Constrained PGD Attack Success Rate: {pgd_integer_success_rate:.2f}%\")\n",
        "print(f\"CPGD Attack Success Rate: {cpgd_success_rate:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Dk3z4u0jkDr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original PGD Attack Success Rate: 100.00% \\\\\n",
        "Integer-Constrained PGD Attack Success Rate: 89.42% \\\\\n",
        "CPGD Attack Success Rate: 99.72% \\\\"
      ],
      "metadata": {
        "id": "_7E1tyXe4pf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Comment your results."
      ],
      "metadata": {
        "id": "8ZthtlIVjGHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show that the original PGD attack is incredibly effective, hitting a perfect success rate of 100%. This really highlights how vulnerable the model is to adversarial examples. When we applied the Integer-Constrained PGD attack, we saw a success rate of 89.42%, suggesting that the model has some level of robustness when constraints are added. Interestingly, the CPGD attack achieved a success rate of 99.72%. This means that even with these extra penalty constraints, the model is still quite vulnerable to adversarial perturbations, but those constraints do help to slightly weaken the attack compared to the original PGD."
      ],
      "metadata": {
        "id": "z-fjBhR-vdr2"
      }
    }
  ]
}